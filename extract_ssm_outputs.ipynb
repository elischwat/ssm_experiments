{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Eli Schwat\n",
    "\n",
    "elilouis@uw.edu\n",
    "\n",
    "Created for Professor Michael Brett's CEWA547 Course, Winter 2021\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract WQM Outputs at WA Ecology Monitoring Stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook extracts sub-daily water quality model (WQM) outputs produced by the Salish Sea Model.  The model generates an output file \n",
    "\n",
    "```\n",
    "SSM_2014_v2.7hyak_chk/WQM/SSM_2014_DO_Ph_T52/outputs/ssm_station.out\n",
    "```\n",
    "\n",
    "which contains outputs at 26 monitoring stations that represent a variety of environmental conditons. \n",
    "\n",
    "To run this notebook, you must provide the path to a copy of that output file. The notebook will create a CSV of results. The saved results are a small subset of available outputs, including\n",
    "\n",
    "Identifying variables:\n",
    "```\n",
    "    StationID\n",
    "    Node\n",
    "    Layer\n",
    "    depth\n",
    "```\n",
    "Simulated Values:\n",
    "```\n",
    "    DO\n",
    "    NO3\n",
    "    NH4\n",
    "    T\n",
    "    S\n",
    "```\n",
    "\n",
    "The data is saved in a wide format, with identifying and simulated variables all represented in columns along with a time index. A gzipped CSV is saved alongside the identified output file.\n",
    "\n",
    "User input is required when you see this...\n",
    "\n",
    "**<span style=\"color:red\">USER INPUT REQUIRED</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">USER INPUT REQUIRED</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs/ssm_station3.out\n",
      "outputs/ssm_station0.out\n",
      "outputs/ssm_station8_shortened.out\n",
      "outputs/ssm_station15.out\n",
      "outputs/ssm_stationNEW60.out\n",
      "outputs/ssm_stationNEW15.out\n",
      "outputs/ssm_station60.out\n",
      "outputs/ssm_stationNEW3.out\n",
      "outputs/ssm_stationNEW0.out\n",
      "outputs/ssm_stationNEW32.out\n",
      "outputs/ssm_stationNEW25.out\n",
      "outputs/ssm_stationNEW8.out\n",
      "outputs/ssm_station25.out\n",
      "outputs/ssm_stationNEW120.out\n",
      "outputs/ssm_station32.out\n",
      "outputs/ssm_station8.out\n",
      "outputs/ssm_station120.out\n"
     ]
    }
   ],
   "source": [
    "!find outputs -type f -name \"*.out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stations = 35\n",
    "input_file = (\n",
    "    \"/Users/elischwat/Google Drive/UW/Classes Winter 2021/Watershed MGMT/salish sea model/code/outputs/ssm_stationNEW120.out\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_file, mode='r') as fp:\n",
    "    df = pd.DataFrame(fp.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_variable_names_from_line(line):\n",
    "    return line[0].replace(\"Variables=\", \"\").replace(\"\\\"\", \"\").split(\",\")\n",
    "variables_list = get_list_of_variable_names_from_line(df.iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['StationID',\n",
       " 'Node',\n",
       " 'Layer',\n",
       " 'depth(m)',\n",
       " 'DO',\n",
       " 'NO3',\n",
       " 'NH4',\n",
       " 'Alg1',\n",
       " 'Alg2',\n",
       " 'LDOC',\n",
       " 'RDOC',\n",
       " 'LPOC',\n",
       " 'RPOC',\n",
       " 'PO4',\n",
       " 'DIC',\n",
       " 'TALK',\n",
       " 'pH',\n",
       " 'pCO2',\n",
       " 'T',\n",
       " 'S',\n",
       " 'P1',\n",
       " 'P2',\n",
       " 'BM1',\n",
       " 'BM2',\n",
       " 'NL1',\n",
       " 'NL2',\n",
       " 'PL1',\n",
       " 'PL2',\n",
       " 'FI1',\n",
       " 'FI2',\n",
       " 'B1SZ',\n",
       " 'B2SZ',\n",
       " 'B1LZ',\n",
       " 'B2LZ',\n",
       " 'PR1',\n",
       " 'PR2',\n",
       " 'IAVG',\n",
       " 'DICUPT',\n",
       " 'DICBMP',\n",
       " 'DICPRD',\n",
       " 'DICMNL',\n",
       " 'DICDEN',\n",
       " 'DICGAS',\n",
       " 'DICSED',\n",
       " 'DICADV',\n",
       " 'DICVDIF',\n",
       " 'ALKNH4',\n",
       " 'ALKNO3',\n",
       " 'ALKNIT',\n",
       " 'ALKDEN',\n",
       " 'ALKREM',\n",
       " 'ALKNH4SED',\n",
       " 'ALKNO3SED',\n",
       " 'ALKADV',\n",
       " 'ALKVDIF',\n",
       " 'Jcin1',\n",
       " 'Jcin2',\n",
       " 'Jcin3',\n",
       " 'Jnin1',\n",
       " 'Jnin2',\n",
       " 'Jnin3',\n",
       " 'Jpin1',\n",
       " 'Jpin2',\n",
       " 'Jpin3',\n",
       " 'Jsin',\n",
       " 'O20',\n",
       " 'Depth',\n",
       " 'Tw',\n",
       " 'NH30',\n",
       " 'NO30',\n",
       " 'PO40',\n",
       " 'SI0',\n",
       " 'CH40',\n",
       " 'SALw',\n",
       " 'SOD',\n",
       " 'Jnh4',\n",
       " 'Jno3',\n",
       " 'JDenitT',\n",
       " 'Jch4',\n",
       " 'Jch4g',\n",
       " 'Jhs',\n",
       " 'Jpo4',\n",
       " 'Jsi',\n",
       " 'NH31',\n",
       " 'NH32',\n",
       " 'NO31',\n",
       " 'NO32',\n",
       " 'PO41',\n",
       " 'PO42',\n",
       " 'Si1',\n",
       " 'Si2',\n",
       " 'CH41',\n",
       " 'CH42',\n",
       " 'HS1',\n",
       " 'HS2',\n",
       " 'POC21',\n",
       " 'POC22',\n",
       " 'POC23',\n",
       " 'PON21',\n",
       " 'PON22',\n",
       " 'PON23',\n",
       " 'POP21',\n",
       " 'POP22',\n",
       " 'POP23',\n",
       " 'POS2',\n",
       " 'H1',\n",
       " 'BEN_STR\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[3:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop empty lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[0].str.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_timestamp_chunk(df):\n",
    "    \"\"\"\n",
    "    Split a df with data for one timestamp into a list of dataframes, \n",
    "    one df per station\n",
    "    \"\"\"\n",
    "    df['time'] =  df[0].iloc[0]\n",
    "    df = df.iloc[1:]\n",
    "    one_timestamp_df_list = np.array_split(df, len(df)/76)\n",
    "    return one_timestamp_df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_station_chunk(df):\n",
    "    \"\"\"\n",
    "    Convert a df for one station and one time into a wide format.\n",
    "    \"\"\"\n",
    "    top_layers_df_list = np.array_split(df[:-13], 9)\n",
    "    \n",
    "    bottom_layer_df = df.iloc[-13:]\n",
    "    bottom_layer_data_list = ' '.join(bottom_layer_df[0]).split()\n",
    "    index, values = zip(*zip(variables_list, bottom_layer_data_list))\n",
    "    bottom_layer_df = pd.DataFrame(values, index=index).transpose()\n",
    "    time = df.time.iloc[0]\n",
    "    top_layers_df = pd.DataFrame()\n",
    "    for df in top_layers_df_list:\n",
    "        index, values = zip(*zip(variables_list, ' '.join(df[0]).split()))\n",
    "        top_layers_df = top_layers_df.append(pd.DataFrame(values, index=index).transpose())\n",
    "    return_df = top_layers_df.append(bottom_layer_df)\n",
    "    return_df['time'] = time\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_length = n_stations*76+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2661"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step_df_list = np.array_split(df, len(df)/chunk_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PROCS = 7\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def process_single_df(df):\n",
    "    station_df_list = process_one_timestamp_chunk(df)\n",
    "    all_stations_one_date_df = pd.concat([\n",
    "        process_one_station_chunk(station_df)\n",
    "        for station_df in station_df_list\n",
    "\n",
    "    ])\n",
    "    return all_stations_one_date_df\n",
    "\n",
    "pool = Pool(processes=NUM_PROCS)\n",
    "\n",
    "allDfs = pool.map(process_single_df, time_step_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat(allDfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df[[\"StationID\",\"Node\",\"Layer\",\"depth(m)\",\"DO\",\"NO3\",\"NH4\",\"T\",\"S\",'time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['time'] = final_df['time'].apply(lambda x: x.split()[2]).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['StationID'] = final_df['StationID'].astype('int')\n",
    "final_df['Node'] \t  = final_df['Node'].astype('int')\n",
    "final_df['Layer'] \t  = final_df['Layer'].astype('int')\n",
    "final_df['depth(m)']  = final_df['depth(m)'].astype('float')\n",
    "final_df['DO'] \t\t  = final_df['DO'].astype('float')\n",
    "final_df['NO3'] \t  = final_df['NO3'].astype('float')\n",
    "final_df['NH4'] \t  = final_df['NH4'].astype('float')\n",
    "final_df['T'] \t\t  = final_df['T'].astype('float')\n",
    "final_df['S'] \t\t  = final_df['S'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\n",
    "    input_file.replace(\".out\", \".csv.gz\"),\n",
    "    compression=\"gzip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
